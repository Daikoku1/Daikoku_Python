{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from PyPDF2 import PdfFileReader\n",
    "import PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.pdfinterp import PDFResourceManager, PDFPageInterpreter\n",
    "from pdfminer.converter import TextConverter\n",
    "from pdfminer.layout import LAParams\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "from io import StringIO\n",
    "\n",
    "def convert_pdf_to_txt(file):\n",
    "    \n",
    "    rsrcmgr = PDFResourceManager()\n",
    "    \n",
    "    retstr = StringIO()\n",
    "    \n",
    "    codec = 'utf-8'\n",
    "    laparams = LAParams()\n",
    "    device = TextConverter(rsrcmgr, retstr, codec=codec, laparams = laparams)\n",
    "    fp = open(file, 'rb')\n",
    "    interpreter = PDFPageInterpreter(rsrcmgr, device)\n",
    "    password = ''\n",
    "    maxpages = 0\n",
    "    caching = True\n",
    "    pagenos = set()\n",
    "    \n",
    "    for page in PDFPage.get_pages(fp, pagenos, maxpages=maxpages, password = password, caching=caching, check_extractable = True):\n",
    "        interpreter.process_page(page)\n",
    "    \n",
    "    text = retstr.getvalue()\n",
    "    fp.close()\n",
    "    device.close()\n",
    "    retstr.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "from PyPDF2 import pdf\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'find'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-13b8a9b701b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mli_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mnew_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mlink_li\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mli_list\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'div'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclass_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'fileGoupBox'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ul'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'li'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mlink\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mlink_li\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlink\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'title'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'pdf'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'find'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "# result_list = []\n",
    "for i in range(1, 45):\n",
    "    b_url = 'http://www.bok.or.kr/portal/bbs/B0000245/list.do?menuNo=200761'\n",
    "    params = {\n",
    "    'pageIndex' : i\n",
    "    }\n",
    "    resp = requests.get(b_url, params = params)\n",
    "    soup = BeautifulSoup(resp.content)\n",
    "    li_list = []\n",
    "    li= soup.find('div', class_='bdLine type2').find('ul').find('li')\n",
    "    li_li = li.find_next_siblings('li')\n",
    "    li_list.append(li)\n",
    "    li_list.extend(li_li)\n",
    "    \n",
    "#     print(len(li_list))\n",
    "#     print(li_list)\n",
    "    print(i)\n",
    "    result_list = []\n",
    "    for x in range(0, len(li_list)):\n",
    "        new_dict = {}\n",
    "        link_li = li_list[x].find('div', class_='fileGoupBox').find('ul').find_all('li')\n",
    "        for link in link_li:\n",
    "            if link.find('a').attrs['title'][-3:] == 'pdf':\n",
    "                link_u = link.find('a').attrs['href']\n",
    "                title = li_list[x].find('div', class_='row').find('span').find('a').find('span').find('span').text\n",
    "                url2 = 'http://www.bok.or.kr' + link_u\n",
    "                file_res2 = requests.get(url2)\n",
    "                file_name = '{}.pdf'.format(title)\n",
    "                with open(file_name, 'wb') as f:\n",
    "                    f.write(file_res2.content)\n",
    "                try:\n",
    "                    text = convert_pdf_to_txt(file_name)\n",
    "                    new_dict = {\n",
    "                        'title' : title,\n",
    "                        'text' : text\n",
    "                    }\n",
    "                    result_list.append(new_dict)\n",
    "\n",
    "                except:\n",
    "                    print('빠진 목록 : {}'.format(title))\n",
    "                    \n",
    "                              \n",
    "    df = pd.DataFrame(result_list)\n",
    "    BASE_DIR = os.getcwd()\n",
    "    SAVE_DB_DIR = os.path.join(BASE_DIR, '금통위')\n",
    "\n",
    "    if not os.path.exists(SAVE_DB_DIR):\n",
    "        os.makedirs(SAVE_DB_DIR)\n",
    "\n",
    "    #텍스트 파일 만들기\n",
    "    temp_name = f\"{title}.xlsx\"\n",
    "    txt_name = os.path.join(SAVE_DB_DIR, temp_name)\n",
    "    excel_writer = pd.ExcelWriter(txt_name, engine = 'xlsxwriter')\n",
    "\n",
    "    df.to_excel(excel_writer)\n",
    "\n",
    "    excel_writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 실패한거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "for i in range(1, 45):\n",
    "    b_url = 'http://www.bok.or.kr/portal/bbs/B0000245/list.do?menuNo=200761'\n",
    "    params = {\n",
    "    'pageIndex' : i\n",
    "    }\n",
    "    resp = requests.get(b_url, params = params)\n",
    "    soup = BeautifulSoup(resp.content)\n",
    "    li_list = []\n",
    "    li= soup.find('div', class_='bdLine type2').find('ul').find('li')\n",
    "    li_li = li.find_next_siblings('li')\n",
    "    li_list.append(li)\n",
    "    li_list.extend(li_li)\n",
    "    \n",
    "#     print(len(li_list))\n",
    "#     print(li_list)\n",
    "    print(i)\n",
    "    for x in range(0, len(li_list)):\n",
    "\n",
    "        try:\n",
    "            new_dict = {}\n",
    "            link_li = li_list[x].find('div', class_='fileGoupBox').find('ul').find_all('li')\n",
    "            for link in link_li:\n",
    "                if link.find('a').attrs['title'][-3:] == 'pdf':\n",
    "                    link_u = link.find('a').attrs['href']\n",
    "                    title = li_list[x].find('div', class_='row').find('span').find('a').find('span').find('span').text\n",
    "                    url2 = 'http://www.bok.or.kr' + link_u\n",
    "                    file_res2 = requests.get(url2)\n",
    "                    file_name = '{}.pdf'.format(title)\n",
    "                    with open(file_name, 'wb') as f:\n",
    "                        f.write(file_res2.content)\n",
    "                    try:\n",
    "                        text = getTextPDF(file_name)\n",
    "                        new_dict = {\n",
    "                            'title' : title,\n",
    "                            'text' : text\n",
    "                        }\n",
    "                        df = pd.DataFrame(new_dict)\n",
    "                        BASE_DIR = os.getcwd()\n",
    "                        SAVE_DB_DIR = os.path.join(BASE_DIR, '금통위')\n",
    "\n",
    "                        if not os.path.exists(SAVE_DB_DIR):\n",
    "                            os.makedirs(SAVE_DB_DIR)\n",
    "\n",
    "                        #텍스트 파일 만들기\n",
    "                        temp_name = f\"{title}.xlsx\"\n",
    "                        txt_name = os.path.join(SAVE_DB_DIR, temp_name)\n",
    "                        excel_writer = pd.ExcelWriter(txt_name, engine = 'xlsxwriter')\n",
    "                \n",
    "                        df.to_excel(excel_writer)\n",
    "                        \n",
    "                        excel_writer.save()\n",
    "                    except:\n",
    "                        print('빠진 목록 : {}'.format(title))\n",
    "        except:\n",
    "            link2 = li_list[x].find('div').find('div').find('a').attrs['href']\n",
    "            title = li_list[x].find('div', class_='row').find('span').find('a').find('span').find('span').text\n",
    "            url3 = 'http://www.bok.or.kr' + link2\n",
    "            file_res3 = requests.get(url3)\n",
    "            \n",
    "            with open('{}.hwp'.format(title), 'wb') as f:\n",
    "                f.write(file_res3.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "convert_pdf_to_txt(file):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b_url = 'http://www.bok.or.kr/portal/bbs/B0000245/list.do?menuNo=200761'\n",
    "params = {\n",
    "'pageIndex' : 1\n",
    "}\n",
    "resp = requests.get(b_url, params = params)\n",
    "soup = BeautifulSoup(resp.content)\n",
    "li_list = []\n",
    "li= soup.find('div', class_='bdLine type2').find('ul').find('li')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li.find('div', class_='fileGoupBox').find('ul').find_all('li')[0].find('a').attrs['title'][-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib import request\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_url = 'http://www.bok.or.kr/portal/cmmn/file/fileDown.do?menuNo=200761&atchFileId=FILE_000000000012003&fileSn=1'\n",
    "a = urlopen(c_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_c = requests.get(c_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(resp_c.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = soup.find('div', class_='fileGoupBox')\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link_li = li_list[x].find('div', class_='fileGoupBox').find('ul').find_all('li')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('34.pdf', 'wb') as f:\n",
    "    resp_c.raw.decode_content = True\n",
    "    shutil.copyfileobj(resp_c.raw, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = request.urlopen(c_url).read()\n",
    "# date2unicode(euc_bytes,'euc-kr').encode('utf-8')\n",
    "\n",
    "text = data.decode('euc-kr')\n",
    "\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " convert_pdf_to_txt(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = request.urlretrieve(c_url)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " convert_pdf_to_txt(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(resp.raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b_url = 'http://www.bok.or.kr/portal/bbs/B0000245/list.do?menuNo=200761'\n",
    "params = {\n",
    "'pageIndex' : 15\n",
    "}\n",
    "resp = requests.get(b_url, params = params)\n",
    "soup = BeautifulSoup(resp.content)\n",
    "li_list = []\n",
    "li= soup.find('div', class_='bdLine type2').find('ul').find('li')\n",
    "li_li = li.find_next_siblings('li')\n",
    "li_list.append(li)\n",
    "li_list.extend(li_li)\n",
    "\n",
    "#     print(len(li_list))\n",
    "#     print(li_list)\n",
    "result_list = []\n",
    "for x in range(0, len(li_list)):\n",
    "    new_dict = {}\n",
    "    link_li = li_list[x].find('div', class_='fileGoupBox').find('ul').find_all('li')\n",
    "    for link in link_li:\n",
    "        if link.find('a').attrs['title'][-3:] == 'pdf':\n",
    "            link_u = link.find('a').attrs['href']\n",
    "            title = li_list[x].find('div', class_='row').find('span').find('a').find('span').find('span').text\n",
    "            url2 = 'http://www.bok.or.kr' + link_u\n",
    "            file_res2 = requests.get(url2)\n",
    "            file_name = '{}.pdf'.format(title)\n",
    "            with open(file_name, 'wb') as f:\n",
    "                f.write(file_res2.content)\n",
    "\n",
    "            text = convert_pdf_to_txt(file_name)\n",
    "            new_dict = {\n",
    "                'title' : title,\n",
    "                'text' : text\n",
    "            }\n",
    "            result_list.append(new_dict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
