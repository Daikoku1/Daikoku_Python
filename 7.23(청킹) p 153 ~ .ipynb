{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 청킹, 문장 구문 분석, 의존성  \n",
    " - 청킹은 텍스트에서 짧은 구를 추출하는 과정\n",
    " - 덩이짓기(청킹) 은 정보를 의미있는 묶음으로 만드는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Namsan', 'Botanical', 'Garden', 'is', 'a', 'well', 'known', 'botanical', 'gardenin', 'Seoul', ',', 'Korea', '.']\n",
      "----------------------------------------\n",
      "[('Namsan', 'NNP'), ('Botanical', 'NNP'), ('Garden', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('well', 'RB'), ('known', 'VBN'), ('botanical', 'JJ'), ('gardenin', 'NN'), ('Seoul', 'NNP'), (',', ','), ('Korea', 'NNP'), ('.', '.')]\n",
      "----------------------------------------\n",
      "(S\n",
      "  (PERSON Namsan/NNP)\n",
      "  (PERSON Botanical/NNP Garden/NNP)\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  well/RB\n",
      "  known/VBN\n",
      "  botanical/JJ\n",
      "  gardenin/NN\n",
      "  (GPE Seoul/NNP)\n",
      "  ,/,\n",
      "  (GPE Korea/NNP)\n",
      "  ./.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "text = \"Namsan Botanical Garden is a well known botanical gardenin Seoul, Korea.\"\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    print(words)\n",
    "    print('-'*40)\n",
    "    tags = nltk.pos_tag(words)\n",
    "    print(tags)\n",
    "    print('-'*40)\n",
    "    chunks = nltk.ne_chunk(tags)\n",
    "    print(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# grammar를 이용하여 청크 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Ravi/NNP)\n",
      "  is/VBZ\n",
      "  (NP the/DT CEO/NNP)\n",
      "  of/IN\n",
      "  (NP a/DT Company/NNP)\n",
      "  ./.)\n",
      "(S\n",
      "  He/PRP\n",
      "  is/VBZ\n",
      "  very/RB\n",
      "  (NP powerful/JJ public/JJ speaker/NN)\n",
      "  also/RB\n",
      "  ./.)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "text = \"Ravi is the CEO of a Company. He is very powerful public speaker also.\"\n",
    "grammar = '\\n'.join([\n",
    " 'NP: {<DT>*<NNP>}', #DT : 한정사 , NNP : 고유명사 => DT가 0번 이상 출현\n",
    " 'NP: {<JJ>*<NN>}', #JJ : 형용사 , NN : 명사 => JJ가 0번 이상 출현\n",
    " 'NP: {<NNP>+}',\n",
    "])\n",
    "sentences = nltk.sent_tokenize(text)\n",
    "for sentence in sentences:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    tags = nltk.pos_tag(words)\n",
    "    chunkparser = nltk.RegexpParser(grammar)\n",
    "    result = chunkparser.parse(tags)\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training 청커"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package treebank is already up-to-date!\n",
      "[nltk_data] Downloading package conll2000 to\n",
      "[nltk_data]     C:\\Users\\student\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package conll2000 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ChunkParse score:\n",
      "    IOB Accuracy:  38.6%%\n",
      "    Precision:      0.0%%\n",
      "    Recall:         0.0%%\n",
      "    F-Measure:      0.0%%\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  48.2%%\n",
      "    Precision:     71.1%%\n",
      "    Recall:        17.2%%\n",
      "    F-Measure:     27.7%%\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  45.0%%\n",
      "    Precision:      0.0%%\n",
      "    Recall:         0.0%%\n",
      "    F-Measure:      0.0%%\n",
      "ChunkParse score:\n",
      "    IOB Accuracy:  50.7%%\n",
      "    Precision:     51.9%%\n",
      "    Recall:         8.8%%\n",
      "    F-Measure:     15.1%%\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('treebank')\n",
    "nltk.download('conll2000')\n",
    "from nltk.corpus import conll2000\n",
    "from nltk.corpus import treebank_chunk\n",
    "def mySimpleChunker() :\n",
    "    grammar = 'NP: {<NNP>+}'\n",
    "    return nltk.RegexpParser(grammar)\n",
    "def test_nothing(data) :\n",
    "    cp = nltk.RegexpParser(\"\")\n",
    "    print(cp.evaluate(data))\n",
    "\n",
    "def test_mysimplechunker(data) :\n",
    "    schunker = mySimpleChunker()\n",
    "    print(schunker.evaluate(data))\n",
    "\n",
    "datasets = [\n",
    "    conll2000.chunked_sents('test.txt', chunk_types=['NP']),\n",
    "    treebank_chunk.chunked_sents()\n",
    "]\n",
    "for dataset in datasets :\n",
    "    test_nothing(dataset[:50])\n",
    "    test_mysimplechunker(dataset[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 회귀  \n",
    " - 재귀 하향 파서는 왼쪽에서 오른쪽으로 입력을 읽고 파생트리를 하향식으로 작성하고 전위순호(pre-order)방\n",
    "노드를 통과시켜는 파서\n",
    " - 컴파일러를 작성하는데 사용\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP (NNP Bangalore) (VBZ is))\n",
      "  (VP (DT the) (NN capital) (IN of) (NNP Karnataka)))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "def RDParserExample(grammar, textlist):\n",
    "    parser = nltk.parse.RecursiveDescentParser(grammar)\n",
    "    for text in textlist:\n",
    "        sentence = nltk.word_tokenize(text)\n",
    "    for tree in parser.parse(sentence):\n",
    "        print(tree)\n",
    "        tree.draw()\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "NP -> NNP VBZ\n",
    "VP -> IN NNP | DT NN IN NNP\n",
    "NNP -> 'Tajmahal' | 'Agra' | 'Bangalore' | 'Karnataka'\n",
    "VBZ -> 'is'\n",
    "IN -> 'in' | 'of'\n",
    "DT -> 'the'\n",
    "NN -> 'capital'\n",
    "\"\"\")\n",
    "text = [\n",
    " \"Tajmahal is in Agra\",\n",
    " \"Bangalore is the capital of Karnataka\",\n",
    "]\n",
    "RDParserExample(grammar, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 시프트 변환구문  \n",
    " - 입력 텍스트에서 첫번째 토큰을 읽고 스택에 넣음\n",
    " - 스택의 전체 구문 분석 트리를 읽고 생성규칙을 오른쪽에서 왼쪽으로 읽음으로써 적용 가능한 생성 규칙을\n",
    "이 과정은 생성규칙이 바닥 날때까지 반복되고, 구문분석이 실패했다는 점을 인정\n",
    " - 이 과정은 입력이 소모될 때까지 반복되며, 구문 분석이 성공했다고 말함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bangalore', 'is', 'the', 'capital', 'of', 'Karnataka']\n",
      "['Tajmahal', 'is', 'in', 'Agra']\n",
      "(S (NP (NNP Tajmahal) (VBZ is)) (VP (IN in) (NNP Agra)))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "def SRParserExample(grammar, textlist):\n",
    "    parser = nltk.parse.ShiftReduceParser(grammar)\n",
    "    for text in textlist:\n",
    "        sentence = nltk.word_tokenize(text)\n",
    "        print(sentence)\n",
    "        for tree in parser.parse(sentence):\n",
    "            print(tree)\n",
    "            tree.draw()\n",
    "text = [\n",
    "    \"Bangalore is the capital of Karnataka\",\n",
    "    \"Tajmahal is in Agra\"\n",
    "]\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "S -> NP VP\n",
    "NP -> NNP VBZ\n",
    "VP -> IN NNP | DT NN IN NNP\n",
    "NNP -> 'Tajmahal' | 'Agra' | 'Bangalore' | 'Karnataka'\n",
    "VBZ -> 'is'\n",
    "IN -> 'in' | 'of'\n",
    "DT -> 'the'\n",
    "NN -> 'capital'\n",
    "\"\"\")\n",
    "\n",
    "SRParserExample(grammar, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 의존 문법과 투사 의존성 구문 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(yield (savings small) (gains large))\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "grammar = nltk.grammar.DependencyGrammar.fromstring(\"\"\"\n",
    "'savings' -> 'small'\n",
    "'yield' -> 'savings'\n",
    "'gains' -> 'large'\n",
    "'yield' -> 'gains'\n",
    "\"\"\")\n",
    "sentence = 'small savings yield large gains'\n",
    "dp = nltk.parse.ProjectiveDependencyParser(grammar)\n",
    "for t in sorted(dp.parse(sentence.split())):\n",
    "    print(t)\n",
    "    t.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(yield (savings small) (gains large))\n"
     ]
    }
   ],
   "source": [
    "for t in dp.parse(sentence.split()):\n",
    "    print(t)\n",
    "    t.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['small', 'savings', 'yield', 'large', 'gains']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gains', 'large', 'savings', 'small', 'yield']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(sentence.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 차트 구문 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|.Bangal.  is  . the  .capita.  of  .Karnat.|\n",
      "|[------]      .      .      .      .      .| [0:1] 'Bangalore'\n",
      "|.      [------]      .      .      .      .| [1:2] 'is'\n",
      "|.      .      [------]      .      .      .| [2:3] 'the'\n",
      "|.      .      .      [------]      .      .| [3:4] 'capital'\n",
      "|.      .      .      .      [------]      .| [4:5] 'of'\n",
      "|.      .      .      .      .      [------]| [5:6] 'Karnataka'\n",
      "|[------]      .      .      .      .      .| [0:1] NNP -> 'Bangalore' *\n",
      "|[------>      .      .      .      .      .| [0:1] T1 -> NNP * VBZ\n",
      "|.      [------]      .      .      .      .| [1:2] VBZ -> 'is' *\n",
      "|[-------------]      .      .      .      .| [0:2] T1 -> NNP VBZ *\n",
      "|[------------->      .      .      .      .| [0:2] S  -> T1 * T4\n",
      "|.      .      [------]      .      .      .| [2:3] DT -> 'the' *\n",
      "|.      .      [------>      .      .      .| [2:3] T2 -> DT * NN\n",
      "|.      .      .      [------]      .      .| [3:4] NN -> 'capital' *\n",
      "|.      .      [-------------]      .      .| [2:4] T2 -> DT NN *\n",
      "|.      .      [------------->      .      .| [2:4] T4 -> T2 * T3\n",
      "|.      .      .      .      [------]      .| [4:5] IN -> 'of' *\n",
      "|.      .      .      .      [------>      .| [4:5] T3 -> IN * NNP\n",
      "|.      .      .      .      .      [------]| [5:6] NNP -> 'Karnataka' *\n",
      "|.      .      .      .      .      [------>| [5:6] T1 -> NNP * VBZ\n",
      "|.      .      .      .      [-------------]| [4:6] T3 -> IN NNP *\n",
      "|.      .      .      .      [-------------]| [4:6] T4 -> T3 *\n",
      "|.      .      [---------------------------]| [2:6] T4 -> T2 T3 *\n",
      "|[=========================================]| [0:6] S  -> T1 T4 *\n",
      "Total Edges : 24\n",
      "(S\n",
      "  (T1 (NNP Bangalore) (VBZ is))\n",
      "  (T4 (T2 (DT the) (NN capital)) (T3 (IN of) (NNP Karnataka))))\n"
     ]
    }
   ],
   "source": [
    "from nltk.grammar import CFG\n",
    "from nltk.parse.chart import ChartParser, BU_LC_STRATEGY\n",
    "grammar = CFG.fromstring(\"\"\"\n",
    "S -> T1 T4\n",
    "T1 -> NNP VBZ\n",
    "T2 -> DT NN\n",
    "T3 -> IN NNP\n",
    "T4 -> T3 | T2 T3\n",
    "NNP -> 'Tajmahal' | 'Agra' | 'Bangalore' | 'Karnataka'\n",
    "VBZ -> 'is'\n",
    "IN -> 'in' | 'of'\n",
    "DT -> 'the'\n",
    "NN -> 'capital'\n",
    "\"\"\")\n",
    "cp = ChartParser(grammar, BU_LC_STRATEGY, trace=True)\n",
    "sentence = \"Bangalore is the capital of Karnataka\"\n",
    "tokens = sentence.split()\n",
    "chart = cp.chart_parse(tokens)\n",
    "parses = list(chart.parses(grammar.start()))\n",
    "print(\"Total Edges :\", len(chart.edges()))\n",
    "for tree in parses: print(tree)\n",
    "tree.draw()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
